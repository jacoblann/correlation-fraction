{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVDsHcizCamw",
    "outputId": "74d9fe16-0a6d-4b11-9b67-2ed5dba91619"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.43)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfhzeQLV0J0o"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    raise ImportError(\"Cannot start without 'yfinance' package.\\nInstall it before running the code again.\")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from collections import OrderedDict\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "class SP500DataLoader:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists('Data'):\n",
    "            os.makedirs('Data')\n",
    "\n",
    "        self.start_date, self.end_date = None, None\n",
    "        self.cleaned_prices = None\n",
    "        self.cleaned_returns = None\n",
    "        self.raw_prices = None\n",
    "        self.raw_returns = None\n",
    "\n",
    "        # Download stocks names from S&P500 page on wikipedia\n",
    "        resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "        table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "        self.tickers = []\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            ticker = row.findAll('td')[0].text\n",
    "            self.tickers.append(ticker)\n",
    "        self.tickers = [s.replace('\\n', '') for s in self.tickers]\n",
    "        self.tickers = self.tickers + [\"SPY\"]\n",
    "\n",
    "    # Check whether 'start_date' and 'end_date' make a valid date range or not\n",
    "    def check_date_range(self, start_date: tuple, end_date: tuple):\n",
    "        start = datetime(*start_date)\n",
    "        end = datetime(*end_date)\n",
    "\n",
    "        if end <= start:\n",
    "            raise Exception(\"The start date must be before the end date!\")\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    # Doanload prices using yfinance\n",
    "    def download_prices(self, start_date, end_date, interval='1d', column='Adj Close'):\n",
    "        self.start_date, self.end_date = self.check_date_range(start_date, end_date)\n",
    "        self.raw_prices = yf.download(self.tickers, start=self.start_date, end=self.end_date, interval=interval)[column]\n",
    "\n",
    "    # Helper function to write dataframes on files with specified names\n",
    "    def write_on_disk(self, data: DataFrame, filename: str):\n",
    "        if \"csv\" in filename:\n",
    "            data.to_csv('Data/' + filename)\n",
    "        elif \"h5\" in filename:\n",
    "            data.to_hdf('Data/' + filename, 'fixed', mode='w', complib='blosc', complevel=9)\n",
    "\n",
    "        print(f\"Saved: Data/{filename}\")\n",
    "\n",
    "    # Return a list of stock names in S&P 500 index\n",
    "    def get_stocks_list(self):\n",
    "        return self.tickers.copy()\n",
    "\n",
    "    # Return raw prices data (which is not cleaned)\n",
    "    def get_raw_prices(self, start_date: tuple, end_date: tuple, interval='1d', column='Adj Close', save_as_h5=False, save_as_csv=False):\n",
    "        self.download_prices(start_date, end_date)\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.raw_prices, \"S&P500-raw_prices.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.raw_prices, \"S&P500-raw_prices.h5\")\n",
    "\n",
    "        return self.raw_prices\n",
    "\n",
    "    # Calculate and return raw returns data (which is not cleaned)\n",
    "    def get_raw_returns(self, start_date: tuple, end_date: tuple, interval='1d', column='Adj Close', save_as_h5=False, save_as_csv=False):\n",
    "        self.get_raw_prices(start_date, end_date)\n",
    "\n",
    "        self.raw_returns = self.raw_prices.copy()\n",
    "        self.raw_returns = np.log(self.raw_returns).diff()\n",
    "        self.raw_returns = self.raw_returns.iloc[1:]  # removes first row which is NaN after diff()\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.raw_returns, \"S&P500-raw_returns.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.raw_returns, \"S&P500-raw_returns.h5\")\n",
    "\n",
    "        return self.raw_returns\n",
    "\n",
    "    # Return cleaned prices data (stocks with at least on NAN value are excluded)\n",
    "    def get_cleaned_prices(self, start_date: tuple, end_date: tuple, interval='1d', column='Adj Close', save_as_h5=False, save_as_csv=False):\n",
    "        self.get_raw_prices(start_date, end_date)\n",
    "\n",
    "        self.cleaned_prices = self.raw_prices.copy()\n",
    "        # Remove companies (columns) with all missing values for whole time range\n",
    "        self.cleaned_prices.dropna(axis='columns', how='all', inplace=True)\n",
    "        # Remove days (rows) with missing values for all of companies\n",
    "        self.cleaned_prices.dropna(axis='index', how='all', inplace=True)\n",
    "        # Finally, remove the columns with at least one Nan (missing value)\n",
    "        self.cleaned_prices.dropna(axis='columns', how='any', inplace=True)\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.self.cleaned_prices, \"S&P500-cleaned_prices.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.self.cleaned_prices, \"S&P500-cleaned_prices.h5\")\n",
    "\n",
    "        return self.cleaned_prices\n",
    "\n",
    "    # Calculate return values using cleaned data, and return the dataframe\n",
    "    def get_cleaned_returns(self, start_date: tuple, end_date: tuple, interval='1d', column='Adj Close', save_as_h5=False, save_as_csv=False):\n",
    "        self.get_cleaned_prices(start_date, end_date)\n",
    "\n",
    "        self.cleaned_returns = self.cleaned_prices.copy()\n",
    "        self.cleaned_returns = np.log(self.cleaned_returns).diff()\n",
    "        self.cleaned_returns = self.cleaned_returns.iloc[1:]  # removes first row which is NaN after diff()\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.cleaned_returns, \"S&P500-cleaned_returns.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.cleaned_returns, \"S&P500-cleaned_returns.h5\")\n",
    "\n",
    "        return self.cleaned_returns\n",
    "\n",
    "    # Return the last values for raw prices without redownloading them\n",
    "    def get_last_raw_prices(self, save_as_h5=False, save_as_csv=False):\n",
    "        if self.raw_prices is None:\n",
    "            return None\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.raw_prices, \"S&P500-raw_prices.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.raw_prices, \"S&P500-raw_prices.h5\")\n",
    "\n",
    "        return self.raw_prices\n",
    "\n",
    "    # Return the last values for raw returns without redownloading them\n",
    "    def get_last_raw_returns(self, save_as_h5=False, save_as_csv=False):\n",
    "        if self.raw_returns is None:\n",
    "            return None\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.raw_returns, \"S&P500-raw_returns.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.raw_returns, \"S&P500-raw_returns.h5\")\n",
    "\n",
    "        return self.raw_returns\n",
    "\n",
    "    # Return the last values for cleaned prices without redownloading them\n",
    "    def get_last_cleaned_prices(self, save_as_h5=False, save_as_csv=False):\n",
    "        if self.cleaned_prices is None:\n",
    "            return None\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.cleaned_prices, \"S&P500-cleaned_prices.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.cleaned_prices, \"S&P500-cleaned_prices.h5\")\n",
    "\n",
    "        return self.cleaned_prices\n",
    "\n",
    "    # Return the last values for cleaned returns without redownloading them\n",
    "    def get_last_cleaned_returns(self, save_as_h5=False, save_as_csv=False):\n",
    "        if self.cleaned_returns is None:\n",
    "            return None\n",
    "\n",
    "        if save_as_csv:\n",
    "            self.write_on_disk(self.cleaned_returns, \"S&P500-cleaned_returns.csv\")\n",
    "        if save_as_h5:\n",
    "            self.write_on_disk(self.cleaned_returns, \"S&P500-cleaned_returns.h5\")\n",
    "\n",
    "        return self.cleaned_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Alex B. estimating specific variances\n",
    "def params_sample_covar(Y, c:int=1):\n",
    "    p, n = Y.shape\n",
    "\n",
    "    # Dual Sample Covariance Eigenvales and Vectors\n",
    "    L = Y.T @ Y/p\n",
    "    vals, vecs = eigsh (np.array(L), 3, which='LA')\n",
    "\n",
    "    vals = vals[::-1]\n",
    "    vecs = np.fliplr(vecs)\n",
    "\n",
    "    # Convert PC's from Dual Covariance\n",
    "    H = Y @ vecs / np.sqrt(p * vals)\n",
    "    sH = H\n",
    "    eigs = vals * p / n\n",
    "\n",
    "    # Select top eigenvector and eigenvalue\n",
    "    h = H.iloc[:,0]\n",
    "    if (sum(h) < 0):\n",
    "        H = -H\n",
    "\n",
    "    # Apply penalty correction to 1 if desired\n",
    "    beta_hat_raw  = c*h + (1-c)\n",
    "\n",
    "    # Residuals of PCA as regression\n",
    "    phi = Y.T @ beta_hat_raw / (beta_hat_raw.T @ beta_hat_raw)\n",
    "    E = Y - np.outer(beta_hat_raw,phi)\n",
    "\n",
    "    # Diagonals of squared residuals for svar\n",
    "    svar_fixed = np.diag(E @ E.T)/n\n",
    "    if(svar_fixed<=0).any(): # Sign for sanity check\n",
    "        print('Problem!')\n",
    "\n",
    "    # Compute factor\n",
    "    fvar_resid = phi.T @ phi / n\n",
    "\n",
    "    # Normalize factor so that beta is mean 1\n",
    "    fvar_fixed = fvar_resid * (beta_hat_raw.mean()**2)\n",
    "    beta_hat_fixed = beta_hat_raw/beta_hat_raw.mean()\n",
    "    return beta_hat_fixed, fvar_fixed, svar_fixed\n",
    "\n",
    "\n",
    "def base_model_sample_covar(Y, n_factors):\n",
    "    '''\n",
    "    constructs \"base\" model with 6 factors where N is number of samples\n",
    "    '''\n",
    "    p,n = Y.shape\n",
    "    L = Y.T @ Y/p\n",
    "    # n_factors = 6\n",
    "\n",
    "    vals, vecs = eigsh (np.array(L), n_factors, which='LA')\n",
    "    vals = vals[::-1]\n",
    "    vecs = np.fliplr(vecs)\n",
    "\n",
    "    # Convert PC's from Dual Covariance\n",
    "    B = Y @ vecs / np.sqrt(p * vals)\n",
    "    eigs = vals * p / n\n",
    "\n",
    "    phi = np.linalg.solve(B.T @ B, B.T @ Y)\n",
    "    E = Y.values - B @ phi\n",
    "    svar = np.diag(E @ E.T)/n\n",
    "\n",
    "    # Set first factor to be mean 1\n",
    "    m1 = B[:,0].mean()\n",
    "    eigs[0] *= m1**2\n",
    "    B[:,0] = B[:,0]/m1\n",
    "\n",
    "    return B, eigs, svar"
   ],
   "metadata": {
    "id": "4p1pFK9Ghi2-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Use PCA and LW constant correlation matrix and try different numbers of factors**",
   "metadata": {
    "id": "yPSTSO5PfQxz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Upper tri masking\n",
    "def upper_tri_masking(A):\n",
    "    m = A.shape[0]\n",
    "    r = np.arange(m)\n",
    "    mask = r[:,None] < r\n",
    "    return A[mask]\n",
    "\n",
    "\n",
    "def diff_per(pc_fraction, avg_corr):\n",
    "    return (pc_fraction - avg_corr) / ((pc_fraction + avg_corr) / 2)\n",
    "\n",
    "\n",
    "def diff(pc_fraction, avg_corr):\n",
    "    return pc_fraction - avg_corr\n",
    "\n",
    "\n",
    "def generate_sequence(number):\n",
    "    # Start with 0 to 4\n",
    "    sequence = list(range(10))\n",
    "    # Add multiples of 100 up to the number\n",
    "    sequence.extend(range(99, number, 100))\n",
    "    return [i + 1 for i in sequence]\n",
    "\n",
    "\n",
    "def generate_arithmetic_array(start=1, step=25, limit=500):\n",
    "    # Initialize the array with the first number\n",
    "    arith_array = [start]\n",
    "    # Continue the arithmetic trend until we reach the limit\n",
    "    num = start + step - 1  # Adjust the first increment to make the second number 25\n",
    "    while num <= limit:\n",
    "        arith_array.append(num)\n",
    "        num += step\n",
    "\n",
    "    return arith_array\n",
    "\n",
    "\n",
    "def generate_exponential_array(limit=500):\n",
    "    # Start with the first number\n",
    "    num = 1\n",
    "        # Initialize the array with the first number\n",
    "    exp_array = [num]\n",
    "        # Continue the exponential trend until we reach around the limit\n",
    "    while num * 2 <= limit:\n",
    "        num *= 2\n",
    "        exp_array.append(num)\n",
    "    return exp_array\n",
    "\n",
    "\n",
    "def shrinkage(returns, cov):\n",
    "    \"\"\"Shrinks sample covariance matrix towards constant correlation unequal variance matrix.\n",
    "\n",
    "    Ledoit & Wolf (\"Honey, I shrunk the sample covariance matrix\", Portfolio Management, 30(2004),\n",
    "    110-119) optimal asymptotic shrinkage between 0 (sample covariance matrix) and 1 (constant\n",
    "    sample average correlation unequal sample variance matrix).\n",
    "\n",
    "    Paper:\n",
    "    http://www.ledoit.net/honey.pdf\n",
    "\n",
    "    Matlab code:\n",
    "    https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n",
    "\n",
    "    Special thanks to Evgeny Pogrebnyak https://github.com/epogrebnyak\n",
    "\n",
    "    :param returns:\n",
    "        n, p - returns of n observations of p stocks.\n",
    "    :return:\n",
    "        Covariance matrix, sample average correlation, shrinkage.\n",
    "    \"\"\"\n",
    "    n, p = returns.shape\n",
    "\n",
    "    # The sample covariance calculated below is very similar to the one I calculated in main function, so use the main function one directly\n",
    "    # mean_returns = np.mean(returns, axis=0, keepdims=True)\n",
    "    # returns -= mean_returns\n",
    "    # sample_cov = returns.transpose() @ returns / n\n",
    "\n",
    "    sample_cov = cov\n",
    "\n",
    "    # sample average correlation\n",
    "    var = np.diag(sample_cov).reshape(-1, 1)\n",
    "    sqrt_var = var ** 0.5\n",
    "    unit_cor_var = sqrt_var * sqrt_var.transpose()\n",
    "    average_cor = ((sample_cov / unit_cor_var).sum() - p) / p / (p - 1)\n",
    "    prior = average_cor * unit_cor_var\n",
    "    np.fill_diagonal(prior, var)\n",
    "\n",
    "    # pi-hat\n",
    "    y = returns ** 2\n",
    "    phi_mat = (y.transpose() @ y) / n - sample_cov ** 2\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # rho-hat\n",
    "    theta_mat = ((returns ** 3).transpose() @ returns) / n - var * sample_cov\n",
    "    np.fill_diagonal(theta_mat, 0)\n",
    "    rho = (\n",
    "        np.diag(phi_mat).sum()\n",
    "        + average_cor * (1 / sqrt_var @ sqrt_var.transpose() * theta_mat).sum()\n",
    "    )\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.linalg.norm(sample_cov - prior, \"fro\") ** 2\n",
    "\n",
    "    # shrinkage constant\n",
    "    kappa = (phi - rho) / gamma\n",
    "    shrink = max(0, min(1, kappa / n))\n",
    "    # print(\"Shrinkage Constant:\", shrink)\n",
    "\n",
    "    # estimator\n",
    "    sigma = shrink * prior + (1 - shrink) * sample_cov\n",
    "    print(\"Shrinkage Constant:\", shrink)\n",
    "\n",
    "    return sigma, prior\n",
    "\n",
    "\n",
    "def ndarray_to_diagonal_matrix(arr):\n",
    "    if arr.ndim != 1:\n",
    "        raise ValueError(\"Input array must be one-dimensional\")\n",
    "    size = arr.shape[0]\n",
    "    # Create a zero matrix of size (n, n)\n",
    "    matrix = np.zeros((size, size))\n",
    "    # Place the original values on the diagonal\n",
    "    np.fill_diagonal(matrix, arr)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def base_model_sample_covar(Y, vecs, n_factors):\n",
    "    '''\n",
    "    constructs \"base\" model with 6 factors where N is number of samples\n",
    "    '''\n",
    "    # p, n = Y.shape\n",
    "    # L = Y.T @ Y/p\n",
    "    # # n_factors = 6\n",
    "\n",
    "    # vals, vecs = eigsh (np.array(L), n_factors, which='LA')\n",
    "    # vals = vals[::-1]\n",
    "    # vecs = np.fliplr(vecs)\n",
    "\n",
    "    # # Convert PC's from Dual Covariance\n",
    "    # B = Y @ vecs / np.sqrt(p * vals)\n",
    "    # eigs = vals * p / n\n",
    "\n",
    "    Y = Y.T\n",
    "    p, n = Y.shape\n",
    "\n",
    "    phi = np.linalg.solve(vecs.T @ vecs, vecs.T @ Y)\n",
    "    # E = Y.values - vecs @ phi\n",
    "    E = Y - vecs @ phi\n",
    "    svar = np.diag(E @ E.T)/n\n",
    "    svar_matrix = ndarray_to_diagonal_matrix(svar)\n",
    "\n",
    "    # Diagonals of squared residuals for svar\n",
    "    svar = np.diag(E @ E.T)/n\n",
    "    if(svar<=0).any(): # Sign for sanity check\n",
    "        print('Problem!')\n",
    "\n",
    "    # Set first factor to be mean 1\n",
    "    # m1 = B[:,0].mean()\n",
    "    # eigs[0] *= m1**2\n",
    "    # B[:,0] = B[:,0]/m1\n",
    "\n",
    "    return svar_matrix\n",
    "\n",
    "\n",
    "def covariance_to_array(cov_matrix, mean_vector, num_samples):\n",
    "    # Perform Cholesky decomposition\n",
    "    L = np.linalg.cholesky(cov_matrix)\n",
    "\n",
    "    # Generate random samples\n",
    "    num_variables = len(mean_vector)\n",
    "    random_samples = np.random.standard_normal((num_samples, num_variables))\n",
    "\n",
    "    # Transform the samples\n",
    "    transformed_samples = np.dot(random_samples, L.T) + mean_vector\n",
    "\n",
    "    return transformed_samples\n",
    "\n",
    "\n",
    "def estimate_correlation_matrix(data, cov, num_components):\n",
    "    \"\"\"\n",
    "    Estimate correlation matrix from the covariance matrix\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    # data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "    n, p = data.shape\n",
    "\n",
    "    eigenvalues, eigenvectors = eigsh(cov, p, which='LM')\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Select top n_components\n",
    "    selected_eigenvectors = eigenvectors[:, :num_components]\n",
    "\n",
    "    # Select top eigenvalues\n",
    "    selected_eigenvalues = ndarray_to_diagonal_matrix(eigenvalues[:num_components])\n",
    "    rest_eigenvalues = eigenvalues[num_components:]\n",
    "    # print(\"Rest Eigenvalues:\", rest_eigenvalues)\n",
    "\n",
    "    # Estimate the covariance matrix using the principal components\n",
    "    estimated_covariance_matrix = np.dot(np.dot(selected_eigenvectors, selected_eigenvalues), selected_eigenvectors.T)\n",
    "\n",
    "    # Subtraction method estimate specfic covariance matrix\n",
    "    subtra_matrix = ndarray_to_diagonal_matrix(np.diag(cov - estimated_covariance_matrix))\n",
    "    hetero_matrix = estimated_covariance_matrix + subtra_matrix\n",
    "\n",
    "    # Estimate the homogeneous specific variances on diagonal\n",
    "    if len(rest_eigenvalues) == 0:\n",
    "        avg_rest_eigenvalues = 0\n",
    "    else:\n",
    "        avg_rest_eigenvalues = np.mean(rest_eigenvalues)\n",
    "    rest_eigenvalues_matrix = (avg_rest_eigenvalues * n / p) * np.identity(cov.shape[1])\n",
    "    homo_matrix = estimated_covariance_matrix + rest_eigenvalues_matrix\n",
    "\n",
    "    hetero_std_devs = np.sqrt(np.diag(hetero_matrix))\n",
    "    hetero_corr_matrix = hetero_matrix / np.outer(hetero_std_devs, hetero_std_devs)\n",
    "    # print(\"Hetero Corr Matrix:\\n\", hetero_corr_matrix)\n",
    "\n",
    "    homo_std_devs = np.sqrt(np.diag(homo_matrix))\n",
    "    homo_corr_matrix = homo_matrix / np.outer(homo_std_devs, homo_std_devs)\n",
    "\n",
    "    return hetero_corr_matrix, homo_corr_matrix\n",
    "\n",
    "\n",
    "# Sequence for testing\n",
    "sequence_original = generate_sequence(400)\n",
    "sequence_target_range = range(200, 300, 2)\n",
    "sequence_linear = generate_arithmetic_array(1, 25, 475)\n",
    "total_linear = list(i + 1 for i in range(500))\n",
    "every_two_linear = generate_arithmetic_array(1, 2, 500)\n",
    "sequence_exp = generate_exponential_array(500) + [497]\n",
    "\n",
    "sequence = sequence_exp\n",
    "\n",
    "# Initiate average correlation list and PC1 fraction list\n",
    "corr_res_dict = {i: {key: [] for key in sequence} for i in range(6)}\n",
    "diff_res_dict = {i: {key: [] for key in sequence} for i in range(6)}\n",
    "pc1_fraction_list = []\n",
    "diffPer_actual = []\n",
    "corr_avg_actual = []\n",
    "\n",
    "# Get cleaned return values from 2000/05/19 to 2023/12/30\n",
    "range_sp500 = range(2023, 2024)\n",
    "\n",
    "for year in range_sp500:\n",
    "    data_downloader_object = SP500DataLoader()\n",
    "    cleaned_returns = data_downloader_object.get_cleaned_returns(\n",
    "        start_date=(year, 1, 1), end_date=(year, 12, 31),\n",
    "        interval='1d', column='Adj Close'\n",
    "        )\n",
    "    print(\"\\nYear, Days and Securities: \", year, cleaned_returns.shape, '\\n')\n",
    "\n",
    "    # Center the data\n",
    "    data_centered = cleaned_returns - np.mean(cleaned_returns, axis=0)\n",
    "\n",
    "    cov = cleaned_returns.cov()\n",
    "    centered_cov = data_centered.cov()\n",
    "    corr = cleaned_returns.corr()\n",
    "    centered_corr = data_centered.corr()\n",
    "    p = data_centered.shape[1]\n",
    "    n = data_centered.shape[0]\n",
    "    upper_tril_array = upper_tri_masking(corr.to_numpy())\n",
    "    tril_avg = np.average(upper_tril_array)\n",
    "\n",
    "    print(\"N:\", n)\n",
    "    print(\"P:\", p)\n",
    "    vals, vecs = eigsh(np.array(cov), p, which='LM')\n",
    "    vals_sorted = vals[::-1]\n",
    "    pc1 = vals_sorted[0]\n",
    "    l_squared = (np.sum(vals_sorted) - pc1)/(n - 1)\n",
    "    delta_squared = l_squared*(n/p)\n",
    "    pc1_fraction = (pc1 - delta_squared)/np.trace(cov)\n",
    "\n",
    "    avg_dict = {0:{}, 1:{}, 2:{}, 3:{}, 4:{}, 5:{}}\n",
    "    for m in sequence:\n",
    "        if m <= p:\n",
    "            print(\"\\n\", \"Factor Number:\", m)\n",
    "            # Three kinds of matrices: Sample Covariance Matrix, LW Estimator Matrix and LW Constant Correlation Matrix\n",
    "            lw_estimator_matrix, lw_constant_corr_matrix = shrinkage(data_centered.to_numpy(), centered_cov.to_numpy())\n",
    "\n",
    "            # Three respective correlation matrices\n",
    "            # print(\"Original:\")\n",
    "            # sample_hetero_corr_matrix, sample_homo_corr_matrix = estimate_correlation_matrix(cleaned_returns.to_numpy(), cov.to_numpy(), m)\n",
    "            # print(\"Original Average:\", np.average(upper_tri_masking(sample_hetero_corr_matrix)))\n",
    "            sample_hetero_corr_matrix, sample_homo_corr_matrix = estimate_correlation_matrix(data_centered.to_numpy(), centered_cov.to_numpy(), m)\n",
    "            lw_estimator_hetero_corr_matrix, lw_estimator_homo_corr_matrix = estimate_correlation_matrix(data_centered.to_numpy(), lw_estimator_matrix, m)\n",
    "            lw_constant_hetero_corr_matrix, lw_constant_homo_corr_matrix = estimate_correlation_matrix(data_centered.to_numpy(), lw_constant_corr_matrix, m)\n",
    "            list_ = [sample_hetero_corr_matrix, sample_homo_corr_matrix, lw_constant_hetero_corr_matrix, lw_constant_homo_corr_matrix, lw_estimator_hetero_corr_matrix, lw_estimator_homo_corr_matrix]\n",
    "\n",
    "            for i in range(6):\n",
    "                avg_dict[i][m] = np.average(upper_tri_masking(list_[i]))\n",
    "\n",
    "        else:\n",
    "            print(\"There is a None\")\n",
    "            for i in range(6):\n",
    "                avg_dict[i][m] = None\n",
    "\n",
    "    # diffPer_array = {}\n",
    "    diff_dict = {0:{}, 1:{}, 2:{}, 3:{}, 4:{}, 5:{}}\n",
    "    for m in sequence:\n",
    "        if m <= p:\n",
    "            # Calculate Diff Percentage\n",
    "            for i in range(6):\n",
    "                diff_dict[i][m] = diff(pc1_fraction, avg_dict[i][m])\n",
    "        else:\n",
    "            for i in range(6):\n",
    "                diff_dict[i][m] = None\n",
    "\n",
    "\n",
    "    pc1_fraction_list.append(pc1_fraction)\n",
    "    corr_avg_actual.append(tril_avg)\n",
    "    diffPer_actual.append(diff(pc1_fraction, tril_avg))\n",
    "\n",
    "    for i in range(6):\n",
    "        for m in sequence:\n",
    "            corr_res_dict[i][m].append(avg_dict[i][m])\n",
    "            diff_res_dict[i][m].append(diff_dict[i][m])\n",
    "\n",
    "# Initiate result data\n",
    "data = OrderedDict()\n",
    "\n",
    "data[\"PC1: Fraction of Variance Explained\"] = pc1_fraction_list\n",
    "\n",
    "for i in range(6):\n",
    "    for m in sequence:\n",
    "        column_name = f\"{i}_{m} Factor AvgCorr\"\n",
    "        data[column_name] = corr_res_dict[i][m]\n",
    "\n",
    "data['Average Correlation'] = corr_avg_actual\n",
    "\n",
    "for i in range(6):\n",
    "    for m in sequence:\n",
    "        column_name = f\"{i}_Diff {m}\"\n",
    "        data[column_name] = diff_res_dict[i][m]\n",
    "\n",
    "data['Diff'] = diffPer_actual\n",
    "\n",
    "linear_df = pd.DataFrame(data, index=list(range_sp500))\n",
    "\n",
    "# Write to excel\n",
    "linear_df.to_excel('all 500.xlsx')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8zizuoNfSYO",
    "outputId": "e352385a-c9da-4d00-d590-bc62684b7747"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[*********************100%***********************]  504 of 504 completed\n",
      "ERROR:yfinance:\n",
      "5 Failed downloads:\n",
      "ERROR:yfinance:['GEV', 'SOLV', 'SW']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2023-01-01 00:00:00 -> 2023-12-31 00:00:00) (Yahoo error = \"Data doesn\\'t exist for startDate = 1672549200, endDate = 1703998800\")')\n",
      "ERROR:yfinance:['BF.B']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2023-01-01 00:00:00 -> 2023-12-31 00:00:00)')\n",
      "ERROR:yfinance:['BRK.B']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Year, Days and Securities:  2023 (249, 497) \n",
      "\n",
      "N: 249\n",
      "P: 497\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-0de58c236af3>:289: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  vals, vecs = eigsh(np.array(cov), p, which='LM')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Factor Number: 1\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-0de58c236af3>:195: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "  eigenvalues, eigenvectors = eigsh(cov, p, which='LM')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Average: 0.24961906883337878\n",
      "Centered:\n",
      "Strand Average: 0.24961906883337887\n",
      "\n",
      " Factor Number: 2\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.2524392160688749\n",
      "Centered:\n",
      "Strand Average: 0.252439216068875\n",
      "\n",
      " Factor Number: 4\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.26342226142170855\n",
      "Centered:\n",
      "Strand Average: 0.26342226142170855\n",
      "\n",
      " Factor Number: 8\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.26345034825936425\n",
      "Centered:\n",
      "Strand Average: 0.26345034825936425\n",
      "\n",
      " Factor Number: 16\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.26348608606977136\n",
      "Centered:\n",
      "Strand Average: 0.2634860860697714\n",
      "\n",
      " Factor Number: 32\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.2634136728607577\n",
      "Centered:\n",
      "Strand Average: 0.26341367286075773\n",
      "\n",
      " Factor Number: 64\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.26323704296180384\n",
      "Centered:\n",
      "Strand Average: 0.2632370429618039\n",
      "\n",
      " Factor Number: 128\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.2630019907967374\n",
      "Centered:\n",
      "Strand Average: 0.2630019907967375\n",
      "\n",
      " Factor Number: 256\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.2628571356146085\n",
      "Centered:\n",
      "Strand Average: 0.2628571356146085\n",
      "\n",
      " Factor Number: 497\n",
      "Shrinkage Constant: 0.21584786181039786\n",
      "Original:\n",
      "Original Average: 0.2628571356146085\n",
      "Centered:\n",
      "Strand Average: 0.2628571356146085\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmQyw4d1wHEu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
